
https://nervanasystems.github.io/distiller/quantization.html

What is Distiller
Distiller is an open-source Python package for neural network compression research.

Network compression can reduce the footprint of a neural network, increase its inference speed and save energy. Distiller provides a PyTorch environment for prototyping and analyzing compression algorithms, such as sparsity-inducing methods and low precision arithmetic.

Distiller contains:

A framework for integrating pruning, regularization and quantization algorithms.
A set of tools for analyzing and evaluating compression performance.
Example implementations of state-of-the-art compression algorithms.