---
layout: post
title: "Introduction to tensors with NumPy"
tags:
- Artficial Intelligence
thumbnail_path: blog/2020-02-23-tensors/scalar-vector-matrix-tensor.png
excerpt: |
  In this blog post my aim is to provide a short yet concise summary of what tensors are. 
---  


### What exactly is a tensor?


What exactly is a tensor?

![tensors](blog/2020-02-23-tensors/scalar-vector-matrix-tensor.png)

A tensor is the basic building block of modern machine learning. In its simplest form, tensors are arrays a multidimensional array to be precise. Think of them as a container of numbers, if you are familiar with NumPy, they are similar to the NumPy's [ndarrays](https://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.html). 

Tensors can also be used on a GPU to accelerate computing. They are a data structure storing a collection of numbers that are accessible individually through an index. Many types of data—from images to time series, audio, and even sentences—can be represented by tensors.

Tensors often offer more natural representations of data, e.g., consider video, which consists of obviously correlated images over time. You can turn this into a matrix, but it's just not natural or intuitive (what does a factorization of some matrix-representation of video mean?).


  - A single-dimensional tensor is represented as a vector.

  - A two-dimensional tensor is represented as a matrix.
  - A three-dimensional tensor => 3D tensor is a cube of numbers[we have to store a number of examples of 2D tensors in their own bucket, which gives us a 3D tensor.]

https://cornebise.com/torch-doc-template/tensor.html

https://www.analyticsvidhya.com/blog/2017/03/tensorflow-understanding-tensors-and-graphs/

https://machinelearningmastery.com/introduction-to-tensors-for-machine-learning/

https://towardsdatascience.com/quick-ml-concepts-tensors-eb1330d7760f

https://hadrienj.github.io/posts/Deep-Learning-Book-Series-2.1-Scalars-Vectors-Matrices-and-Tensors/

https://hackernoon.com/learning-ai-if-you-suck-at-math-p4-tensors-illustrated-with-cats-27f0002c9b32
We can keep stacking cubes together to create bigger and bigger tensors to encode different types of data aka 4D tensors, 5D tensors and so on up to N. N is used by math peeps to define an unknown number of additional units in a set continuing into the future. It could be 5, 10 or a zillion.
Actually, a 3D tensor might be better visualized as a layer of grids, which looks something like the graphic below:


Tensors comes in sizes,there are multiple sizes of tensors. Let’s go through the most basic ones that you’ll run across in deep learning.



We would be using Pytorch deep learning framework in this blogpost, I would be showing you how to implement tensors with it.

From your broswer go to [Colab](https://colab.research.google.com/), Colab allows you to write and execute Python in your web browser, with free access to GPUs no 

configuration is required to use Google colab






In PyTorch, tensors can be declared using the simple Tensor object:

        import numpy
        x = np.array(5)
        print(x)






Information is represented using tensors,